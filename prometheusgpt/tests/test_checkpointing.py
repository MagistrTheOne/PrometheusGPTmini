"""
PrometheusGPT Mini - Checkpointing Tests
Author: MagistrTheOne, Krasnodar, 2025

–¢–µ—Å—Ç—ã –¥–ª—è gradient checkpointing –∏ dynamic attention.
"""

import torch
import pytest
from torch.utils.checkpoint import checkpoint
import time

from src.model import PrometheusGPTMini, model_config, training_config


class TestCheckpointing:
    """–¢–µ—Å—Ç—ã –¥–ª—è gradient checkpointing"""

    def test_gradient_checkpointing_enabled(self):
        """–¢–µ—Å—Ç –≤–∫–ª—é—á–µ–Ω–Ω–æ–≥–æ gradient checkpointing"""

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º checkpointing
        config = model_config
        config.use_gradient_checkpointing = True

        model = PrometheusGPTMini(config)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –±–ª–æ–∫–∏ –∏–º–µ—é—Ç checkpointing
        for layer in model.encoder.layers:
            assert layer.use_gradient_checkpointing == True

        for layer in model.decoder.layers:
            assert layer.use_gradient_checkpointing == True

    def test_gradient_checkpointing_disabled(self):
        """–¢–µ—Å—Ç –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–≥–æ gradient checkpointing"""

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º checkpointing
        config = model_config
        config.use_gradient_checkpointing = False

        model = PrometheusGPTMini(config)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –±–ª–æ–∫–∏ –Ω–µ –∏–º–µ—é—Ç checkpointing
        for layer in model.encoder.layers:
            assert layer.use_gradient_checkpointing == False

        for layer in model.decoder.layers:
            assert layer.use_gradient_checkpointing == False

    def test_memory_usage_comparison(self):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ —Å/–±–µ–∑ checkpointing"""

        batch_size = 16
        seq_len = 256

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        src_tokens = torch.randint(0, 1000, (batch_size, seq_len))
        tgt_tokens = torch.randint(0, 1000, (batch_size, seq_len))

        # –¢–µ—Å—Ç –±–µ–∑ checkpointing
        config1 = model_config
        config1.use_gradient_checkpointing = False

        model1 = PrometheusGPTMini(config1)
        model1.train()

        # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # –ò–∑–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –¥–æ
        if torch.cuda.is_available():
            memory_before = torch.cuda.memory_allocated(0)

        # Forward pass –±–µ–∑ checkpointing
        with torch.no_grad():
            output1 = model1(src_tokens, tgt_tokens)

        if torch.cuda.is_available():
            memory_after = torch.cuda.memory_allocated(0)
            memory_used_no_checkpoint = memory_after - memory_before

        # –¢–µ—Å—Ç —Å checkpointing
        config2 = model_config
        config2.use_gradient_checkpointing = True

        model2 = PrometheusGPTMini(config2)
        model2.train()

        # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # –ò–∑–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –¥–æ
        if torch.cuda.is_available():
            memory_before = torch.cuda.memory_allocated(0)

        # Forward pass —Å checkpointing
        with torch.no_grad():
            output2 = model2(src_tokens, tgt_tokens)

        if torch.cuda.is_available():
            memory_after = torch.cuda.memory_allocated(0)
            memory_used_with_checkpoint = memory_after - memory_before

            # Checkpointing –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
            assert memory_used_with_checkpoint < memory_used_no_checkpoint * 1.5

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ outputs –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É
        assert output1.shape == output2.shape

    def test_training_step_with_checkpointing(self):
        """–¢–µ—Å—Ç —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è —Å checkpointing"""

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å checkpointing
        config = model_config
        config.use_gradient_checkpointing = True

        model = PrometheusGPTMini(config)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size = 8
        seq_len = 128

        src_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        tgt_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

        # Training step
        model.train()
        optimizer.zero_grad()

        # Forward pass
        outputs = model(src_tokens, tgt_tokens)

        # Loss
        loss = torch.mean(outputs ** 2)  # dummy loss

        # Backward pass
        loss.backward()

        # Optimizer step
        optimizer.step()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ loss –∫–æ–Ω–µ—á–Ω—ã–π
        assert torch.isfinite(loss)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        for param in model.parameters():
            if param.requires_grad:
                assert param.grad is not None
                assert torch.isfinite(param.grad).all()

    def test_checkpointing_deterministic(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å checkpointing"""

        # –§–∏–∫—Å–∏—Ä—É–µ–º seed
        torch.manual_seed(42)

        config = model_config
        config.use_gradient_checkpointing = True

        model1 = PrometheusGPTMini(config)
        model2 = PrometheusGPTMini(config)

        # –°–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        torch.manual_seed(42)
        src_tokens1 = torch.randint(0, config.vocab_size, (4, 64))
        tgt_tokens1 = torch.randint(0, config.vocab_size, (4, 64))

        torch.manual_seed(42)
        src_tokens2 = torch.randint(0, config.vocab_size, (4, 64))
        tgt_tokens2 = torch.randint(0, config.vocab_size, (4, 64))

        # Forward pass
        with torch.no_grad():
            output1 = model1(src_tokens1, tgt_tokens1)
            output2 = model2(src_tokens2, tgt_tokens2)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
        assert torch.allclose(output1, output2, rtol=1e-5)


class TestDynamicAttention:
    """–¢–µ—Å—Ç—ã –¥–ª—è dynamic attention pruning"""

    def test_dynamic_attention_config(self):
        """–¢–µ—Å—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ dynamic attention"""

        config = model_config
        config.use_dynamic_attention = True
        config.attention_prune_ratio = 0.3
        config.attention_threshold = 0.2

        model = PrometheusGPTMini(config)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –±–ª–æ–∫–∏ –∏–º–µ—é—Ç dynamic attention
        for layer in model.encoder.layers:
            assert layer.use_dynamic_attention == True

    def test_attention_pruning_shapes(self):
        """–¢–µ—Å—Ç —á—Ç–æ attention pruning –Ω–µ –º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤"""

        config = model_config
        config.use_dynamic_attention = True

        model = PrometheusGPTMini(config)

        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size = 4
        seq_len = 64

        src_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        tgt_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))

        # Forward pass
        with torch.no_grad():
            outputs = model(src_tokens, tgt_tokens)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
        expected_shape = (batch_size, seq_len, config.vocab_size)
        assert outputs.shape == expected_shape

    def test_attention_pruning_toggle(self):
        """–¢–µ—Å—Ç –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è dynamic attention"""

        # –ú–æ–¥–µ–ª—å –±–µ–∑ pruning
        config1 = model_config
        config1.use_dynamic_attention = False

        model1 = PrometheusGPTMini(config1)

        # –ú–æ–¥–µ–ª—å —Å pruning
        config2 = model_config
        config2.use_dynamic_attention = True

        model2 = PrometheusGPTMini(config2)

        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size = 2
        seq_len = 32

        src_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        tgt_tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))

        # Forward pass –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        with torch.no_grad():
            outputs1 = model1(src_tokens, tgt_tokens)
            outputs2 = model2(src_tokens, tgt_tokens)

        # –û–±–µ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        assert outputs1.shape == outputs2.shape


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    print("=== PrometheusGPT Mini Checkpointing Tests ===")
    print("Author: MagistrTheOne, Krasnodar, 2025")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç—ã
    test_checkpointing = TestCheckpointing()
    test_attention = TestDynamicAttention()

    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    print("\n1. Testing gradient checkpointing...")
    test_checkpointing.test_gradient_checkpointing_enabled()
    test_checkpointing.test_gradient_checkpointing_disabled()
    test_checkpointing.test_checkpointing_deterministic()
    print("‚úÖ Checkpointing tests passed!")

    print("\n2. Testing dynamic attention...")
    test_attention.test_dynamic_attention_config()
    test_attention.test_attention_pruning_shapes()
    test_attention.test_attention_pruning_toggle()
    print("‚úÖ Dynamic attention tests passed!")

    print("\n3. Testing memory usage...")
    test_checkpointing.test_memory_usage_comparison()
    print("‚úÖ Memory usage tests passed!")

    print("\n4. Testing training step...")
    test_checkpointing.test_training_step_with_checkpointing()
    print("‚úÖ Training step tests passed!")

    print("\nüéâ All tests passed!")
