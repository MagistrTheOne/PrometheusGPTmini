"""
PrometheusGPT Mini - Advanced Training Pipeline
Author: MagistrTheOne, Krasnodar, 2025

–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π training pipeline —Å gradient checkpointing, monitoring –∏ –≤—Å–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º.
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from torch.nn.utils import clip_grad_norm_
import time
import logging
import json
from typing import Dict, Any, Optional, Tuple, List
from pathlib import Path
from datetime import datetime
import psutil
import threading
import queue
import signal
import sys

from ..model import PrometheusGPTMini, model_config, training_config
from ..data.tokenizer import AdvancedBPETokenizer
from ..data.prepare_dataset import DatasetPreparator
from ..data.dataloader import CachedTextDataset, create_dataloader, get_memory_usage

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TrainingMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""

    def __init__(self, log_interval: int = 10, checkpoint_interval: int = 1000):
        self.log_interval = log_interval
        self.checkpoint_interval = checkpoint_interval
        self.metrics_queue = queue.Queue()
        self.running = True

        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()

    def _monitor_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        while self.running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                if not self.metrics_queue.empty():
                    metrics = self.metrics_queue.get()
                    self._log_metrics(metrics)

                time.sleep(1)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
            except Exception as e:
                logger.error(f"Monitor error: {e}")

    def _log_metrics(self, metrics: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        step = metrics.get('step', 0)
        loss = metrics.get('loss', 0)
        lr = metrics.get('learning_rate', 0)
        step_time = metrics.get('step_time', 0)

        if step % self.log_interval == 0:
            memory = get_memory_usage()

            log_message = (
                f"Step {step:6d} | Loss: {loss:.4f} | LR: {lr:.6f} | "
                f"Time: {step_time:.3f}s | "
            )

            if 'gpu_allocated_gb' in memory:
                log_message += f"GPU: {memory['gpu_allocated_gb']:.2f}GB | "
            else:
                log_message += f"CPU Memory: {memory.get('cpu_only', 'N/A')} | "

            logger.info(log_message)

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.running = False
        if self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)


class AdvancedTrainer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è PrometheusGPT Mini"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Args:
            config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        """

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.model_config = model_config
        self.training_config = training_config

        if config:
            self._update_config(config)

        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = PrometheusGPTMini(self.model_config)
        self.model.to(self.device)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.training_config.learning_rate,
            weight_decay=self.training_config.weight_decay,
            betas=(0.9, 0.999)
        )

        # Scheduler
        self.scheduler = self._create_scheduler()

        # Mixed precision
        self.scaler = GradScaler(enabled=self.training_config.use_mixed_precision)

        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.monitor = TrainingMonitor()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_steps': 0,
            'best_loss': float('inf'),
            'current_epoch': 0,
            'steps_per_epoch': 0
        }

        logger.info(f"Advanced Trainer initialized. Device: {self.device}")

    def _update_config(self, config: Dict[str, Any]):
        """–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        for key, value in config.items():
            if hasattr(self.model_config, key):
                setattr(self.model_config, key, value)
            if hasattr(self.training_config, key):
                setattr(self.training_config, key, value)

    def _create_scheduler(self) -> torch.optim.lr_scheduler.LambdaLR:
        """–°–æ–∑–¥–∞—Ç—å scheduler —Å warmup –∏ cosine annealing"""

        warmup_steps = getattr(self.training_config, 'warmup_steps', 1000)
        num_epochs = getattr(self.training_config, 'num_epochs', 10)

        def lr_lambda(step):
            # Warmup phase
            if step < warmup_steps:
                return float(step) / float(max(1, warmup_steps))

            # Cosine annealing
            progress = (step - warmup_steps) / float(max(1, num_epochs * self.stats['steps_per_epoch'] - warmup_steps))
            return 0.5 * (1.0 + torch.cos(torch.tensor(min(progress, 1.0) * 3.14159)))

        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)

    def save_checkpoint(self, step: int, loss: float, save_path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç"""

        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'step': step,
            'loss': loss,
            'config': {
                'model': self.model_config.__dict__,
                'training': self.training_config.__dict__
            },
            'stats': self.stats,
            'author': 'MagistrTheOne, Krasnodar, 2025',
            'timestamp': datetime.now().isoformat()
        }

        torch.save(checkpoint, save_path)
        logger.info(f"Checkpoint saved: {save_path} (step {step}, loss {loss:.4f})")

    def load_checkpoint(self, checkpoint_path: str) -> int:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç"""

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])

        self.stats.update(checkpoint.get('stats', {}))
        start_step = checkpoint.get('step', 0)

        logger.info(f"Checkpoint loaded: {checkpoint_path} (step {start_step})")

        return start_step

    def train_step(self, batch: Dict[str, torch.Tensor], step: int) -> Dict[str, float]:
        """–û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è"""

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        batch = self._prepare_batch(batch)

        # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        self.optimizer.zero_grad()

        # Forward pass —Å mixed precision
        with autocast(enabled=self.training_config.use_mixed_precision):
            outputs = self.model(batch['input_ids'], batch['target_ids'])
            loss = self._compute_loss(outputs, batch['target_ids'])

        # Backward pass
        self.scaler.scale(loss).backward()

        # Gradient clipping
        if self.training_config.max_grad_norm > 0:
            self.scaler.unscale_(self.optimizer)
            clip_grad_norm_(self.model.parameters(), self.training_config.max_grad_norm)

        # Optimizer step
        self.scaler.step(self.optimizer)
        self.scaler.update()

        # Scheduler step
        self.scheduler.step()

        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'step': step,
            'loss': loss.item(),
            'learning_rate': self.optimizer.param_groups[0]['lr'],
            'step_time': time.time() - self.stats.get('last_step_time', time.time())
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats['last_step_time'] = time.time()
        self.stats['total_steps'] = step

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –º–æ–Ω–∏—Ç–æ—Ä
        self.monitor.metrics_queue.put(metrics)

        return metrics

    def _prepare_batch(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –±–∞—Ç—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        return {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

    def _compute_loss(self, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª–∏—Ç—å loss"""
        batch_size, seq_len, vocab_size = outputs.shape

        # Reshape –¥–ª—è cross-entropy
        outputs = outputs.view(-1, vocab_size)
        targets = targets.view(-1)

        # Cross-entropy loss
        loss = nn.CrossEntropyLoss(ignore_index=0)(outputs, targets)  # ignore PAD

        return loss

    def validate(self, dataloader: torch.utils.data.DataLoader, step: int) -> Dict[str, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""

        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in dataloader:
                batch = self._prepare_batch(batch)

                # Forward pass
                outputs = self.model(batch['input_ids'], batch['target_ids'])
                loss = self._compute_loss(outputs, batch['target_ids'])

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        self.model.train()

        logger.info(f"Validation at step {step}: Loss = {avg_loss:.4f}")

        return {'val_loss': avg_loss, 'step': step}

    def generate_sample(self, prompt: str, max_length: int = 50) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞"""

        self.model.eval()

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        tokens = self.tokenizer.encode(prompt, max_length=self.model_config.max_seq_length)
        input_tensor = torch.tensor([tokens], dtype=torch.long).to(self.device)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
        with torch.no_grad():
            generated_tokens = self.model.generate(
                input_tensor,
                max_length=max_length,
                temperature=0.8,
                top_k=50,
                top_p=0.9,
                do_sample=True
            )

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
        generated_text = self.tokenizer.decode(generated_tokens[0].tolist())

        self.model.train()

        return generated_text

    def train(self, train_dataloader: torch.utils.data.DataLoader,
              val_dataloader: Optional[torch.utils.data.DataLoader] = None,
              num_steps: int = 10000, output_dir: str = "models",
              resume_from: Optional[str] = None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è

        Args:
            train_dataloader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_dataloader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            num_steps: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            resume_from: –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        """

        logger.info("=== Starting Advanced Training ===")
        logger.info(f"Author: MagistrTheOne, Krasnodar, 2025")
        logger.info(f"Target steps: {num_steps}")
        logger.info(f"Device: {self.device}")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        # –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        start_step = 0
        if resume_from:
            start_step = self.load_checkpoint(resume_from)
            logger.info(f"Resuming from step {start_step}")

        # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        self.stats['start_time'] = time.time()

        try:
            for step in range(start_step, num_steps):
                # –ü–æ–ª—É—á–∞–µ–º –±–∞—Ç—á
                try:
                    batch = next(iter(train_dataloader))
                except StopIteration:
                    # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä
                    train_dataloader = self._recreate_dataloader(train_dataloader)
                    batch = next(iter(train_dataloader))

                # –®–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                metrics = self.train_step(batch, step)

                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                if val_dataloader and step % 1000 == 0:
                    val_metrics = self.validate(val_dataloader, step)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
                    if val_metrics['val_loss'] < self.stats['best_loss']:
                        self.stats['best_loss'] = val_metrics['val_loss']
                        self.save_checkpoint(step, val_metrics['val_loss'],
                                           str(output_path / f"best_checkpoint_step_{step}.pt"))

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
                if step % self.monitor.checkpoint_interval == 0:
                    self.save_checkpoint(step, metrics['loss'],
                                       str(output_path / f"checkpoint_step_{step}.pt"))

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä
                if step % 2000 == 0:
                    sample_text = self.generate_sample("–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç")
                    logger.info(f"Sample generation at step {step}: {sample_text[:100]}...")

        except KeyboardInterrupt:
            logger.info("Training interrupted by user")
        except Exception as e:
            logger.error(f"Training error: {e}")
            raise
        finally:
            self.stats['end_time'] = time.time()
            self.monitor.stop()

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
            self.save_checkpoint(num_steps, metrics.get('loss', 0),
                               str(output_path / "final_checkpoint.pt"))

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._save_training_stats(output_path)

        logger.info("=== Training Completed ===")
        logger.info(f"Total steps: {self.stats['total_steps']}")
        logger.info(f"Best loss: {self.stats['best_loss']:.4f}")
        logger.info(f"Total time: {self.stats['end_time'] - self.stats['start_time']:.2f}s")

    def _recreate_dataloader(self, old_dataloader):
        """–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä"""
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫—É
        return old_dataloader

    def _save_training_stats(self, output_path: Path):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è"""

        stats_file = output_path / "training_stats.json"

        training_stats = {
            'author': 'MagistrTheOne, Krasnodar, 2025',
            'timestamp': datetime.now().isoformat(),
            'model_config': self.model_config.__dict__,
            'training_config': self.training_config.__dict__,
            'final_stats': self.stats,
            'device': str(self.device),
            'total_parameters': sum(p.numel() for p in self.model.parameters())
        }

        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(training_stats, f, indent=2, ensure_ascii=False)

        logger.info(f"Training stats saved to {stats_file}")


def create_demo_training_setup():
    """–°–æ–∑–¥–∞—Ç—å –¥–µ–º–æ setup –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""

    from ..data.prepare_dataset import DatasetPreparator
    from ..data.tokenizer import AdvancedBPETokenizer, create_large_demo_dataset

    logger.info("Creating demo training setup...")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    preparator = DatasetPreparator(target_sentences=10000)
    ru_texts, en_texts, train_ru, train_en = preparator.prepare_dataset()

    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = AdvancedBPETokenizer()
    tokenizer.train(ru_texts + en_texts, "demo_tokenizer", vocab_size=3000)

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    train_dataset = CachedTextDataset(train_ru, tokenizer, use_cache=True)
    val_dataset = CachedTextDataset(train_en[:1000], tokenizer, use_cache=True)

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã
    train_dataloader = create_dataloader(train_dataset, batch_size=16, memory_optimized=True)
    val_dataloader = create_dataloader(val_dataset, batch_size=16, memory_optimized=True)

    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = AdvancedTrainer()

    return trainer, train_dataloader, val_dataloader, tokenizer


def run_smoke_test(num_steps: int = 10000):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å smoke test"""

    logger.info(f"=== Starting Smoke Test: {num_steps} steps ===")
    logger.info("Author: MagistrTheOne, Krasnodar, 2025")

    # –°–æ–∑–¥–∞–µ–º setup
    trainer, train_dataloader, val_dataloader, tokenizer = create_demo_training_setup()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    trainer.train(
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        num_steps=num_steps,
        output_dir="models/smoke_test"
    )

    logger.info("=== Smoke Test Completed ===")
    logger.info(f"Total steps: {trainer.stats['total_steps']}")
    logger.info(f"Best loss: {trainer.stats['best_loss']:.4f}")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä
    sample_text = trainer.generate_sample("–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –æ")
    logger.info(f"Final sample: {sample_text}")

    return trainer.stats


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ smoke test
    print("=== PrometheusGPT Mini Advanced Training Pipeline Demo ===")
    print("Author: MagistrTheOne, Krasnodar, 2025")

    # –ó–∞–ø—É—Å–∫–∞–µ–º smoke test –Ω–∞ 1000 —à–∞–≥–æ–≤
    stats = run_smoke_test(1000)

    print(f"\n‚úÖ Smoke test completed!")
    print(f"Total steps: {stats['total_steps']}")
    print(f"Best loss: {stats['best_loss']:.4f}")

    print(f"\nüéâ Phase 4 - Dataset & Training: SMOKE TEST PASSED!")

